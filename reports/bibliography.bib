@article{nathan2015mapping,
  title={Mapping digital businesses with big data: Some early findings from the UK},
  author={Nathan, Max and Rosso, Anna},
  journal={Research Policy},
  volume={44},
  number={9},
  pages={1714--1733},
  year={2015},
  publisher={Elsevier}
}

@article{mateos2014map,
  title={A map of the UK games industry},
  author={Mateos-Garcia, Juan and Bakhshi, Hasan and Lenel, Mark},
  journal={London: Nesta},
  year={2014}
}

@article{nathan2017industrial,
  title={Industrial Clusters in England},
  author={Nathan, Max and Adderley, Simon and Bernini, Michele and Mulhall, Rachel and Ramirez, Paulina},
  year={2017},
  publisher={Department for Business, Energy \& Industrial Strategy}
}

@article{bishop2019exploring,
  title={Exploring the link between economic complexity and emergent economic activities},
  author={Bishop, Alex and Mateos-Garcia, Juan},
  journal={National Institute Economic Review},
  volume={249},
  number={1},
  pages={R47--R58},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@book{bean2016independent,
  title={Independent review of UK economic statistics},
  author={Bean, Charles R},
  year={2016},
  publisher={HM Treasury}
}

@article{hicks2011structural,
  title={Structural change and industrial classification},
  author={Hicks, Diana},
  journal={Structural Change and Economic Dynamics},
  volume={22},
  number={2},
  pages={93--105},
  year={2011},
  publisher={Elsevier}
}

@article{mateos2018immersive,
  title={The immersive economy in the UK: the growth of virtual, augmented, and mixed reality technologies},
  author={Mateos-Garcia, Juan and Stathoulopoulos, Konstantinos and Thomas, Nick},
  year={2018},
  publisher={Nesta, UK Research and Innovation, MTM London, GLASS}
}

@inproceedings{hutto2014vader,
  title={Vader: A parsimonious rule-based model for sentiment analysis of social media text},
  author={Hutto, Clayton and Gilbert, Eric},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={8},
  number={1},
  year={2014}
}

@inproceedings{asano1988clustering,
  title={Clustering algorithms based on minimum and maximum spanning trees},
  author={Asano, Tetsuo and Bhattacharya, Binay and Keil, Mark and Yao, Frances},
  booktitle={Proceedings of the fourth annual symposium on Computational Geometry},
  pages={252--257},
  year={1988}
}

@article{fortunato2016community,
  title={Community detection in networks: A user guide},
  author={Fortunato, Santo and Hric, Darko},
  journal={Physics reports},
  volume={659},
  pages={1--44},
  year={2016},
  publisher={Elsevier}
}

@article{javed2018community,
  title={Community detection in networks: A multidisciplinary review},
  author={Javed, Muhammad Aqib and Younis, Muhammad Shahzad and Latif, Siddique and Qadir, Junaid and Baig, Adeel},
  journal={Journal of Network and Computer Applications},
  volume={108},
  pages={87--111},
  year={2018},
  publisher={Elsevier}
}

@article{rossetti2019cdlib,
  title={CDLIB: a python library to extract, compare and evaluate communities from complex networks},
  author={Rossetti, Giulio and Milli, Letizia and Cazabet, R{\'e}my},
  journal={Applied Network Science},
  volume={4},
  number={1},
  pages={1--26},
  year={2019},
  publisher={Springer}
}

@InProceedings{10.1007/978-3-642-33718-5_31,
author="Zhang, Wei
and Wang, Xiaogang
and Zhao, Deli
and Tang, Xiaoou",
editor="Fitzgibbon, Andrew
and Lazebnik, Svetlana
and Perona, Pietro
and Sato, Yoichi
and Schmid, Cordelia",
title="Graph Degree Linkage: Agglomerative Clustering on a Directed Graph",
booktitle="Computer Vision -- ECCV 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="428--441",
abstract="This paper proposes a simple but effective graph-based agglomerative algorithm, for clustering high-dimensional data. We explore the different roles of two fundamental concepts in graph theory, indegree and outdegree, in the context of clustering. The average indegree reflects the density near a sample, and the average outdegree characterizes the local geometry around a sample. Based on such insights, we define the affinity measure of clusters via the product of average indegree and average outdegree. The product-based affinity makes our algorithm robust to noise. The algorithm has three main advantages: good performance, easy implementation, and high computational efficiency. We test the algorithm on two fundamental computer vision problems: image clustering and object matching. Extensive experiments demonstrate that it outperforms the state-of-the-arts in both applications.",
isbn="978-3-642-33718-5"
}

@article{Traag_2019,
   title={From Louvain to Leiden: guaranteeing well-connected communities},
   volume={9},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-019-41695-z},
   DOI={10.1038/s41598-019-41695-z},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Traag, V. A. and Waltman, L. and van Eck, N. J.},
   year={2019},
   month={Mar}
}

@article{Blondel_2008,
  doi = {10.1088/1742-5468/2008/10/p10008},
  url = {https://doi.org/10.1088/1742-5468/2008/10/p10008},
  year = 2008,
  month = {oct},
  publisher = {{IOP} Publishing},
  volume = {2008},
  number = {10},
  pages = {P10008},
  author = {Vincent D Blondel and Jean-Loup Guillaume and Renaud Lambiotte and Etienne Lefebvre},
  title = {Fast unfolding of communities in large networks},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.}
}

@article{ustalov2019watset,
  title={Watset: Local-global graph clustering with applications in sense and frame induction},
  author={Ustalov, Dmitry and Panchenko, Alexander and Biemann, Chris and Ponzetto, Simone Paolo},
  journal={Computational Linguistics},
  volume={45},
  number={3},
  pages={423--479},
  year={2019},
  publisher={MIT Press}
}

@article{raghavan2007near,
  title={Near linear time algorithm to detect community structures in large-scale networks},
  author={Raghavan, Usha Nandini and Albert, R{\'e}ka and Kumara, Soundar},
  journal={Physical review E},
  volume={76},
  number={3},
  pages={036106},
  year={2007},
  publisher={APS}
}

@article{enright2002efficient,
  title={An efficient algorithm for large-scale detection of protein families},
  author={Enright, Anton J and Van Dongen, Stijn and Ouzounis, Christos A},
  journal={Nucleic acids research},
  volume={30},
  number={7},
  pages={1575--1584},
  year={2002},
  publisher={Oxford University Press}
}

@article{peixoto2014hierarchical,
  title={Hierarchical block structures and high-resolution model selection in large networks},
  author={Peixoto, Tiago P},
  journal={Physical Review X},
  volume={4},
  number={1},
  pages={011047},
  year={2014},
  publisher={APS}
}

@misc{mcdiarmid2018modularity,
    title={Modularity of Erdős-Rényi random graphs},
    author={Colin McDiarmid and Fiona Skerman},
    year={2018},
    eprint={1808.02243},
    archivePrefix={arXiv},
    primaryClass={math.CO}
}

@inproceedings{mikolov2013linguistic,
  title={Linguistic regularities in continuous space word representations},
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages={746--751},
  year={2013}
}

@inproceedings{le2014distributed,
  title={Distributed representations of sentences and documents},
  author={Le, Quoc and Mikolov, Tomas},
  booktitle={International conference on machine learning},
  pages={1188--1196},
  year={2014},
  organization={PMLR}
}

@article{topSBM,
  title={A network approach to topic models},
  author={Gerlach, Martin and Peixoto, Tiago P and Altmann, Eduardo G},
  journal={Science advances},
  volume={4},
  number={7},
  pages={eaaq1360},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{levenshtein,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  booktitle={Soviet physics doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966},
  organization={Soviet Union}
}

@book{mmds,
  title={Mining of massive datasets},
  author={Rajaraman, Anand and Ullman, Jeffrey David},
  year={2011},
  publisher={Cambridge University Press}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2021-03-07},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/George/Zotero/storage/PFEWA4KQ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/George/Zotero/storage/MUER35P5/1706.html:text/html}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-03-07},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/George/Zotero/storage/K2IKAQWQ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/George/Zotero/storage/I46IVXCW/1810.html:text/html}
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2021-03-07},
	journal = {arXiv:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.01108},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/George/Zotero/storage/KJYQXLLD/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:/Users/George/Zotero/storage/6P9PUYT5/1910.html:text/html}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@article{Johnson_2019,
   title={Billion-scale similarity search with GPUs},
   ISSN={2372-2096},
   url={http://dx.doi.org/10.1109/tbdata.2019.2921572},
   DOI={10.1109/tbdata.2019.2921572},
   journal={IEEE Transactions on Big Data},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Johnson, Jeff and Douze, Matthijs and Jegou, Herve},
   year={2019},
   pages={1–1}
}

